{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nội dung thực tập chính:\n",
    " -   Nghiên cứu mô hình hệ thống IDPS cho mạng Hybrid sử dụng công nghệ Machine Learning\n",
    "Sau đây là mô hình cơ bản của mô ML cho hệ thống IDPS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn bị môi trường cơ bản:\n",
    "1. pandas:\n",
    "Được sử dụng để xử lý dữ liệu và thao tác với DataFrame.\n",
    "Cài đặt: pip install pandas\n",
    "2. numpy:\n",
    "Cung cấp các chức năng toán học cơ bản và làm việc với mảng số học.\n",
    "Cài đặt: pip install numpy\n",
    "3. scikit-learn:\n",
    "Thư viện cơ bản cho các thuật toán học máy, tiền xử lý dữ liệu, và đánh giá mô hình.\n",
    "Cài đặt: pip install scikit-learn\n",
    "4. imblearn:\n",
    "Thư viện để xử lý vấn đề dữ liệu không cân bằng, ví dụ như SMOTE.\n",
    "Cài đặt: pip install imbalanced-learn\n",
    "5. matplotlib:\n",
    "Dùng để vẽ đồ thị và biểu đồ.\n",
    "Cài đặt: pip install matplotlib\n",
    "6. seaborn:\n",
    "Thư viện vẽ đồ thị dựa trên matplotlib, cung cấp các chức năng để tạo biểu đồ đẹp và dễ hiểu hơn.\n",
    "Cài đặt: pip install seaborn\n",
    "Tổng hợp lệnh cài như sau:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó đi từng bước để hoàn thiện mô hình:\n",
    "\n",
    "Bước 1: Thu Thập và Chuẩn Bị Dữ Liệu\n",
    "Trong bước này, chúng ta sẽ thực hiện các nhiệm vụ sau:\n",
    "- Tải Dữ Liệu: Sử dụng pandas để tải dữ liệu từ một URL.\n",
    "-  Mã Hóa Dữ Liệu: Chuyển đổi các cột không phải số sang dạng số.\n",
    "- Xử Lý Nhãn: Mã hóa nhãn thành các loại tấn công cụ thể.\n",
    "- Chuẩn Hóa Dữ Liệu: Sử dụng StandardScaler để chuẩn hóa dữ liệu.\n",
    "- Cân Bằng Dữ Liệu: Sử dụng SMOTE để cân bằng các lớp dữ liệu.\n",
    "- Chia Dữ Liệu: Chia dữ liệu thành tập huấn luyện và kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import logging\n",
    "\n",
    "# Cấu hình ghi log\n",
    "logging.basicConfig(filename='model_training.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "error_log_file = 'error_log.txt'\n",
    "\n",
    "def log_error(message):\n",
    "    with open(error_log_file, 'a') as f:\n",
    "        f.write(f\"{message}\\n\")\n",
    "    logging.error(message)\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    try:\n",
    "        url = 'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt'\n",
    "        column_names = [\n",
    "            'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "            'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "            'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "            'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "            'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "            'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "            'dst_host_srv_rerror_rate', 'label'\n",
    "        ]\n",
    "        data = pd.read_csv(url, names=column_names)\n",
    "\n",
    "        # Mã hóa các cột không phải số\n",
    "        categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "        for col in categorical_cols:\n",
    "            data[col] = data[col].astype('category').cat.codes\n",
    "\n",
    "        # Phân chia dữ liệu thành features và labels\n",
    "        x = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "\n",
    "        # Mã hóa nhãn thành các loại tấn công cụ thể\n",
    "        attack_types = {\n",
    "            0: 'normal',\n",
    "            1: 'DoS', 2: 'DoS', 3: 'DoS', 4: 'DoS', 5: 'DoS', 6: 'DoS',\n",
    "            7: 'U2R', 8: 'U2R', 9: 'U2R', 10: 'U2R',\n",
    "            11: 'R2L', 12: 'R2L', 13: 'R2L', 14: 'R2L', 15: 'R2L', 16: 'R2L', 17: 'R2L', 18: 'R2L',\n",
    "            19: 'Probe', 20: 'Probe', 21: 'Probe', 22: 'Probe'\n",
    "        }\n",
    "\n",
    "        # Thay đổi cách mã hóa nhãn\n",
    "        y_mapped = [attack_types.get(label, 'unknown') for label in y]\n",
    "\n",
    "        # Chuẩn hóa dữ liệu\n",
    "        scaler = StandardScaler()\n",
    "        x = scaler.fit_transform(x)\n",
    "\n",
    "        # Cân bằng dữ liệu\n",
    "        smote = SMOTE(random_state=42)\n",
    "        x, y_mapped = smote.fit_resample(x, y_mapped)\n",
    "\n",
    "        # Chia dữ liệu thành tập huấn luyện và kiểm tra\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y_mapped, test_size=0.3, random_state=42,\n",
    "                                                            stratify=y_mapped)\n",
    "        logging.info(\"Data preprocessing complete.\")\n",
    "        return x_train, x_test, y_train, y_test, scaler\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error in load_and_preprocess_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Chạy hàm chuẩn bị dữ liệu\n",
    "x_train, x_test, y_train, y_test, scaler = load_and_preprocess_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phân Tích Bước 1:\n",
    "- Tải Dữ Liệu: Chúng ta sử dụng pandas để tải dữ liệu từ một liên kết công khai. Dữ liệu này chứa các đặc trưng của các phiên mạng và nhãn chỉ loại tấn công.\n",
    "- Mã Hóa Dữ Liệu: Các cột phân loại như protocol_type, service, và flag được mã hóa thành số để dễ dàng xử lý bởi các mô hình machine learning.\n",
    "- Xử Lý Nhãn: Nhãn được chuyển thành các loại tấn công cụ thể để mô hình dễ phân loại.\n",
    "- Chuẩn Hóa Dữ Liệu: StandardScaler được sử dụng để chuẩn hóa dữ liệu, đảm bảo rằng tất cả các đặc trưng có cùng quy mô.\n",
    "- Cân Bằng Dữ Liệu: SMOTE giúp cân bằng số lượng mẫu giữa các lớp để cải thiện hiệu suất của mô hình.\n",
    "- Chia Dữ Liệu: Dữ liệu được chia thành tập huấn luyện và kiểm tra, với tỉ lệ 70% cho huấn luyện và 30% cho kiểm tra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước 2: Xây Dựng và Huấn Luyện Các Mô Hình Machine Learning\n",
    "Trong bước này, chúng ta sẽ:\n",
    "- Chọn Các Mô Hình: Lựa chọn và cấu hình các mô hình machine learning phổ biến như K-Nearest Neighbors, Support Vector Machine, Decision Tree, Random Forest, Neural Networks, Logistic Regression, và Gradient Boosting.\n",
    "- Huấn Luyện Các Mô Hình: Sử dụng GridSearchCV để tìm kiếm các siêu tham số tối ưu cho từng mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_models(x_train, y_train):\n",
    "    try:\n",
    "        models = {}\n",
    "\n",
    "        # K-Nearest Neighbors\n",
    "        knn_params = {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_grid = GridSearchCV(knn, knn_params, cv=5)\n",
    "        knn_grid.fit(x_train, y_train)\n",
    "        models['KNN'] = knn_grid.best_estimator_\n",
    "        logging.info(\"KNN model trained.\")\n",
    "\n",
    "        # Support Vector Machine\n",
    "        svm_params = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': [0.001, 0.01, 0.1, 1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "        svm = SVC(probability=True)\n",
    "        svm_grid = GridSearchCV(svm, svm_params, cv=5)\n",
    "        svm_grid.fit(x_train, y_train)\n",
    "        models['SVM'] = svm_grid.best_estimator_\n",
    "        logging.info(\"SVM model trained.\")\n",
    "\n",
    "        # Decision Tree\n",
    "        dt_params = {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "        dt_grid.fit(x_train, y_train)\n",
    "        models['Decision Tree'] = dt_grid.best_estimator_\n",
    "        logging.info(\"Decision Tree model trained.\")\n",
    "\n",
    "        # Random Forest\n",
    "        rf_params = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "        rf = RandomForestClassifier()\n",
    "        rf_grid = GridSearchCV(rf, rf_params, cv=5)\n",
    "        rf_grid.fit(x_train, y_train)\n",
    "        models['Random Forest'] = rf_grid.best_estimator_\n",
    "        logging.info(\"Random Forest model trained.\")\n",
    "\n",
    "        # Neural Networks\n",
    "        mlp_params = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'sgd']\n",
    "        }\n",
    "        mlp = MLPClassifier(max_iter=300)\n",
    "        mlp_grid = GridSearchCV(mlp, mlp_params, cv=5)\n",
    "        mlp_grid.fit(x_train, y_train)\n",
    "        models['Neural Networks'] = mlp_grid.best_estimator_\n",
    "        logging.info(\"Neural Networks model trained.\")\n",
    "\n",
    "        # Logistic Regression\n",
    "        lr_params = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        lr = LogisticRegression(max_iter=300)\n",
    "        lr_grid = GridSearchCV(lr, lr_params, cv=5)\n",
    "        lr_grid.fit(x_train, y_train)\n",
    "        models['Logistic Regression'] = lr_grid.best_estimator_\n",
    "        logging.info(\"Logistic Regression model trained.\")\n",
    "\n",
    "        # Gradient Boosting\n",
    "        gb_params = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 1]\n",
    "        }\n",
    "        gb = GradientBoostingClassifier()\n",
    "        gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "        gb_grid.fit(x_train, y_train)\n",
    "        models['Gradient Boosting'] = gb_grid.best_estimator_\n",
    "        logging.info(\"Gradient Boosting model trained.\")\n",
    "\n",
    "        return models\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error in train_models: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Huấn luyện các mô hình\n",
    "models = train_models(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phân Tích Bước 2\n",
    "- Chọn Các Mô Hình: Chúng ta đã chọn một loạt các mô hình machine learning phổ biến và cấu hình các siêu tham số mà chúng ta muốn tối ưu hóa.\n",
    "- Huấn Luyện Các Mô Hình: GridSearchCV giúp chúng ta tìm ra các giá trị siêu tham số tốt nhất cho mỗi mô hình bằng cách thực hiện tìm kiếm lưới (grid search) và đánh giá hiệu suất mô hình trên tập dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước 3: Đánh Giá Các Mô Hình\n",
    "Trong bước này, chúng ta sẽ:\n",
    "- Dự Đoán và Đánh Giá: Sử dụng các mô hình để dự đoán trên dữ liệu kiểm tra và tính toán các chỉ số đánh giá như accuracy, classification report, confusion matrix, và ROC AUC.\n",
    "- Vẽ Biểu Đồ: Vẽ confusion matrix và ROC curve để trực quan hóa hiệu suất của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "def evaluate_models(models, x_test, y_test):\n",
    "    try:\n",
    "        for model_name, model in models.items():\n",
    "            y_pred = model.predict(x_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            fpr, tpr, _ = roc_curve(y_test, model.predict_proba(x_test)[:, 1], pos_label='normal')\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            logging.info(f\"{model_name} Accuracy: {accuracy}\")\n",
    "            logging.info(f\"{model_name} Classification Report:\\n{report}\")\n",
    "            logging.info(f\"{model_name} Confusion Matrix:\\n{cm}\")\n",
    "            logging.info(f\"{model_name} ROC AUC: {roc_auc}\")\n",
    "\n",
    "            # Vẽ Confusion Matrix\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "            plt.title(f'Confusion Matrix - {model_name}')\n",
    "            plt.xlabel('Predicted Labels')\n",
    "            plt.ylabel('True Labels')\n",
    "            plt.savefig(f'{model_name}_confusion_matrix.png')\n",
    "            plt.close()\n",
    "\n",
    "            # Vẽ ROC Curve\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'Receiver Operating Characteristic - {model_name}')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.savefig(f'{model_name}_roc_curve.png')\n",
    "            plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error in evaluate_models: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Đánh giá các mô hình\n",
    "evaluate_models(models, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phân Tích Bước 3\n",
    "- Dự Đoán và Đánh Giá: Chúng ta tính toán các chỉ số đánh giá mô hình và ghi lại chúng vào file log. Các chỉ số này giúp chúng ta hiểu được hiệu suất của mô hình trên tập dữ liệu kiểm tra.\n",
    "- Vẽ Biểu Đồ: Các biểu đồ như confusion matrix và ROC curve giúp trực quan hóa hiệu suất của các mô hình, cho phép chúng ta so sánh giữa các mô hình dễ dàng hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước 4: Phát Hiện Xâm Nhập\n",
    "Trong bước này, chúng ta sẽ:\n",
    "- Phát Hiện Xâm Nhập: Sử dụng mô hình đã huấn luyện để dự đoán loại tấn công trên dữ liệu mới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def detect_intrusion(model, scaler, new_data):\n",
    "    try:\n",
    "        new_data = scaler.transform(new_data)\n",
    "        predictions = model.predict(new_data)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error in detect_intrusion: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Ví dụ phát hiện xâm nhập\n",
    "sample_data = np.array([[0.1, 0.2, 0.3, ...]])  # Thay thế bằng dữ liệu mới\n",
    "predictions = detect_intrusion(models['KNN'], scaler, sample_data)\n",
    "print(\"Intrusion Detection Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
